{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55c7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ff80ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "248a25b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input Excel file\n",
    "input_file = \"Input.xlsx\"\n",
    "df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180199db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the extracted article text files\n",
    "output_directory = \"extracted_articles\"\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e63b0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article extracted and saved: extracted_articles\\blackassign0001.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0002.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0003.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0004.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0005.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0006.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0007.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0008.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0009.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0010.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0011.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0012.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0013.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0014.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0015.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0016.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0017.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0018.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0019.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0020.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0021.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0022.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0023.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0024.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0025.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0026.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0027.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0028.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0029.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0030.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0031.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0032.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0033.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0034.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0035.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0036.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0037.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0038.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0039.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0040.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0041.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0042.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0043.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0044.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0045.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0046.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0047.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0048.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0049.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0050.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0051.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0052.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0053.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0054.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0055.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0056.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0057.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0058.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0059.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0060.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0061.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0062.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0063.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0064.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0065.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0066.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0067.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0068.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0069.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0070.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0071.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0072.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0073.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0074.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0075.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0076.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0077.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0078.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0079.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0080.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0081.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0082.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0083.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0084.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0085.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0086.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0087.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0088.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0089.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0090.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0091.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0092.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0093.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0094.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0095.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0096.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0097.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0098.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0099.txt\n",
      "Article extracted and saved: extracted_articles\\blackassign0100.txt\n"
     ]
    }
   ],
   "source": [
    "# Function to extract article text from a URL and save it to a text file\n",
    "def extract_and_save_article(url, url_id):\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Find the article title\n",
    "        title = soup.find('title').get_text().strip()\n",
    "        # Find the article text\n",
    "        article_text = \"\"\n",
    "        for paragraph in soup.find_all('p'):\n",
    "            article_text += paragraph.get_text().strip() + \"\\n\"\n",
    "\n",
    "        # Save the extracted article text to a text file\n",
    "        output_file = os.path.join(output_directory, f\"{url_id}.txt\")\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Title: {title}\\n\\n\")\n",
    "            f.write(article_text)\n",
    "        print(f\"Article extracted and saved: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting article from {url}: {e}\")\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    extract_and_save_article(url, url_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318adc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  for these variables in the analysis document (Text Analysis.docx):\n",
    "#1.\tPOSITIVE SCORE\n",
    "#2.\tNEGATIVE SCORE\n",
    "#3.\tPOLARITY SCORE\n",
    "#4.\tSUBJECTIVITY SCORE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42109181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Positive Score: 15\n",
      "Negative Score: 3\n",
      "Polarity Score: 0.6666666296296316\n",
      "Subjectivity Score: 0.04422604411738073\n",
      "------------------------------\n",
      "Document 2:\n",
      "Positive Score: 66\n",
      "Negative Score: 33\n",
      "Polarity Score: 0.33333332996633\n",
      "Subjectivity Score: 0.08631211849493275\n",
      "------------------------------\n",
      "Document 3:\n",
      "Positive Score: 47\n",
      "Negative Score: 26\n",
      "Polarity Score: 0.28767122893601055\n",
      "Subjectivity Score: 0.07549120984954373\n",
      "------------------------------\n",
      "Document 4:\n",
      "Positive Score: 47\n",
      "Negative Score: 77\n",
      "Polarity Score: -0.24193548191987516\n",
      "Subjectivity Score: 0.1297071128350344\n",
      "------------------------------\n",
      "Document 5:\n",
      "Positive Score: 29\n",
      "Negative Score: 10\n",
      "Polarity Score: 0.4871794746877058\n",
      "Subjectivity Score: 0.0608424336024299\n",
      "------------------------------\n",
      "Document 6:\n",
      "Positive Score: 96\n",
      "Negative Score: 29\n",
      "Polarity Score: 0.5359999957120001\n",
      "Subjectivity Score: 0.08311170207239912\n",
      "------------------------------\n",
      "Document 7:\n",
      "Positive Score: 33\n",
      "Negative Score: 45\n",
      "Polarity Score: -0.1538461518737673\n",
      "Subjectivity Score: 0.0987341770902099\n",
      "------------------------------\n",
      "Document 8:\n",
      "Positive Score: 39\n",
      "Negative Score: 11\n",
      "Polarity Score: 0.5599999888000002\n",
      "Subjectivity Score: 0.06265664152549293\n",
      "------------------------------\n",
      "Document 9:\n",
      "Positive Score: 47\n",
      "Negative Score: 52\n",
      "Polarity Score: -0.050505049994898486\n",
      "Subjectivity Score: 0.10543130979187294\n",
      "------------------------------\n",
      "Document 10:\n",
      "Positive Score: 69\n",
      "Negative Score: 70\n",
      "Polarity Score: -0.007194244552559392\n",
      "Subjectivity Score: 0.12822878216953063\n",
      "------------------------------\n",
      "Document 11:\n",
      "Positive Score: 69\n",
      "Negative Score: 21\n",
      "Polarity Score: 0.5333333274074075\n",
      "Subjectivity Score: 0.0790861159234744\n",
      "------------------------------\n",
      "Document 12:\n",
      "Positive Score: 88\n",
      "Negative Score: 26\n",
      "Polarity Score: 0.5438596443521084\n",
      "Subjectivity Score: 0.09321340957218853\n",
      "------------------------------\n",
      "Document 13:\n",
      "Positive Score: 47\n",
      "Negative Score: 15\n",
      "Polarity Score: 0.5161290239334029\n",
      "Subjectivity Score: 0.11720226820944751\n",
      "------------------------------\n",
      "Document 14:\n",
      "Positive Score: 30\n",
      "Negative Score: 29\n",
      "Polarity Score: 0.016949152255099114\n",
      "Subjectivity Score: 0.06844547555864794\n",
      "------------------------------\n",
      "Document 15:\n",
      "Positive Score: 43\n",
      "Negative Score: 29\n",
      "Polarity Score: 0.19444444174382722\n",
      "Subjectivity Score: 0.07438016521241718\n",
      "------------------------------\n",
      "Document 16:\n",
      "Positive Score: 43\n",
      "Negative Score: 29\n",
      "Polarity Score: 0.19444444174382722\n",
      "Subjectivity Score: 0.07438016521241718\n",
      "------------------------------\n",
      "Document 17:\n",
      "Positive Score: 51\n",
      "Negative Score: 14\n",
      "Polarity Score: 0.569230760473373\n",
      "Subjectivity Score: 0.07198228120489227\n",
      "------------------------------\n",
      "Document 18:\n",
      "Positive Score: 41\n",
      "Negative Score: 12\n",
      "Polarity Score: 0.5471698009967962\n",
      "Subjectivity Score: 0.05955056173084206\n",
      "------------------------------\n",
      "Document 19:\n",
      "Positive Score: 68\n",
      "Negative Score: 8\n",
      "Polarity Score: 0.7894736738227148\n",
      "Subjectivity Score: 0.06249999994860197\n",
      "------------------------------\n",
      "Document 20:\n",
      "Positive Score: 16\n",
      "Negative Score: 2\n",
      "Polarity Score: 0.7777777345679036\n",
      "Subjectivity Score: 0.04063205408435202\n",
      "------------------------------\n",
      "Document 21:\n",
      "Positive Score: 31\n",
      "Negative Score: 49\n",
      "Polarity Score: -0.22499999718750005\n",
      "Subjectivity Score: 0.09411764694809689\n",
      "------------------------------\n",
      "Document 22:\n",
      "Positive Score: 21\n",
      "Negative Score: 12\n",
      "Polarity Score: 0.2727272644628102\n",
      "Subjectivity Score: 0.07894736823218333\n",
      "------------------------------\n",
      "Document 23:\n",
      "Positive Score: 39\n",
      "Negative Score: 17\n",
      "Polarity Score: 0.3928571358418369\n",
      "Subjectivity Score: 0.055500495485133305\n",
      "------------------------------\n",
      "Document 24:\n",
      "Positive Score: 32\n",
      "Negative Score: 5\n",
      "Polarity Score: 0.7297297100073051\n",
      "Subjectivity Score: 0.06618962421075202\n",
      "------------------------------\n",
      "Document 25:\n",
      "Positive Score: 28\n",
      "Negative Score: 13\n",
      "Polarity Score: 0.3658536496133256\n",
      "Subjectivity Score: 0.050679851606081766\n",
      "------------------------------\n",
      "Document 26:\n",
      "Positive Score: 32\n",
      "Negative Score: 16\n",
      "Polarity Score: 0.3333333263888891\n",
      "Subjectivity Score: 0.06022584685040672\n",
      "------------------------------\n",
      "Document 27:\n",
      "Positive Score: 35\n",
      "Negative Score: 28\n",
      "Polarity Score: 0.11111110934744271\n",
      "Subjectivity Score: 0.069459757365535\n",
      "------------------------------\n",
      "Document 28:\n",
      "Positive Score: 42\n",
      "Negative Score: 28\n",
      "Polarity Score: 0.19999999714285718\n",
      "Subjectivity Score: 0.07981755977215786\n",
      "------------------------------\n",
      "Document 29:\n",
      "Positive Score: 74\n",
      "Negative Score: 36\n",
      "Polarity Score: 0.34545454231404965\n",
      "Subjectivity Score: 0.07773851584612118\n",
      "------------------------------\n",
      "Document 30:\n",
      "Positive Score: 69\n",
      "Negative Score: 40\n",
      "Polarity Score: 0.2660550434306877\n",
      "Subjectivity Score: 0.10770750977499258\n",
      "------------------------------\n",
      "Document 31:\n",
      "Positive Score: 74\n",
      "Negative Score: 40\n",
      "Polarity Score: 0.29824561141889816\n",
      "Subjectivity Score: 0.09083665331407437\n",
      "------------------------------\n",
      "Document 32:\n",
      "Positive Score: 72\n",
      "Negative Score: 29\n",
      "Polarity Score: 0.42574257004215277\n",
      "Subjectivity Score: 0.09519321385938433\n",
      "------------------------------\n",
      "Document 33:\n",
      "Positive Score: 67\n",
      "Negative Score: 25\n",
      "Polarity Score: 0.45652173416824204\n",
      "Subjectivity Score: 0.07692307685875996\n",
      "------------------------------\n",
      "Document 34:\n",
      "Positive Score: 53\n",
      "Negative Score: 25\n",
      "Polarity Score: 0.35897435437212366\n",
      "Subjectivity Score: 0.08533916839678428\n",
      "------------------------------\n",
      "Document 35:\n",
      "Positive Score: 36\n",
      "Negative Score: 14\n",
      "Polarity Score: 0.4399999912000002\n",
      "Subjectivity Score: 0.0770416023466231\n",
      "------------------------------\n",
      "Document 36:\n",
      "Positive Score: 9\n",
      "Negative Score: 2\n",
      "Polarity Score: 0.636363578512402\n",
      "Subjectivity Score: 0.0536585363236169\n",
      "------------------------------\n",
      "Document 37:\n",
      "Positive Score: 46\n",
      "Negative Score: 15\n",
      "Polarity Score: 0.5081967129803818\n",
      "Subjectivity Score: 0.10166666649722222\n",
      "------------------------------\n",
      "Document 38:\n",
      "Positive Score: 76\n",
      "Negative Score: 43\n",
      "Polarity Score: 0.277310922039404\n",
      "Subjectivity Score: 0.08439716306071122\n",
      "------------------------------\n",
      "Document 39:\n",
      "Positive Score: 54\n",
      "Negative Score: 64\n",
      "Polarity Score: -0.08474576199367999\n",
      "Subjectivity Score: 0.08200138979708034\n",
      "------------------------------\n",
      "Document 40:\n",
      "Positive Score: 46\n",
      "Negative Score: 24\n",
      "Polarity Score: 0.31428570979591847\n",
      "Subjectivity Score: 0.07726269307145398\n",
      "------------------------------\n",
      "Document 41:\n",
      "Positive Score: 40\n",
      "Negative Score: 30\n",
      "Polarity Score: 0.14285714081632656\n",
      "Subjectivity Score: 0.07128309565042455\n",
      "------------------------------\n",
      "Document 42:\n",
      "Positive Score: 71\n",
      "Negative Score: 26\n",
      "Polarity Score: 0.463917520990541\n",
      "Subjectivity Score: 0.09807886744380297\n",
      "------------------------------\n",
      "Document 43:\n",
      "Positive Score: 75\n",
      "Negative Score: 24\n",
      "Polarity Score: 0.5151515099479645\n",
      "Subjectivity Score: 0.08730158722460177\n",
      "------------------------------\n",
      "Document 44:\n",
      "Positive Score: 35\n",
      "Negative Score: 2\n",
      "Polarity Score: 0.8918918677867064\n",
      "Subjectivity Score: 0.06981132062299751\n",
      "------------------------------\n",
      "Document 45:\n",
      "Positive Score: 87\n",
      "Negative Score: 41\n",
      "Polarity Score: 0.35937499719238286\n",
      "Subjectivity Score: 0.12673267314184883\n",
      "------------------------------\n",
      "Document 46:\n",
      "Positive Score: 29\n",
      "Negative Score: 2\n",
      "Polarity Score: 0.8709677138397511\n",
      "Subjectivity Score: 0.04754601219701532\n",
      "------------------------------\n",
      "Document 47:\n",
      "Positive Score: 23\n",
      "Negative Score: 8\n",
      "Polarity Score: 0.4838709521331951\n",
      "Subjectivity Score: 0.047184170400024095\n",
      "------------------------------\n",
      "Document 48:\n",
      "Positive Score: 9\n",
      "Negative Score: 5\n",
      "Polarity Score: 0.2857142653061239\n",
      "Subjectivity Score: 0.046666666511111114\n",
      "------------------------------\n",
      "Document 49:\n",
      "Positive Score: 9\n",
      "Negative Score: 2\n",
      "Polarity Score: 0.636363578512402\n",
      "Subjectivity Score: 0.0536585363236169\n",
      "------------------------------\n",
      "Document 50:\n",
      "Positive Score: 33\n",
      "Negative Score: 70\n",
      "Polarity Score: -0.359223297483269\n",
      "Subjectivity Score: 0.10098039205786236\n",
      "------------------------------\n",
      "Document 51:\n",
      "Positive Score: 26\n",
      "Negative Score: 17\n",
      "Polarity Score: 0.20930232071389954\n",
      "Subjectivity Score: 0.06825396814562862\n",
      "------------------------------\n",
      "Document 52:\n",
      "Positive Score: 66\n",
      "Negative Score: 40\n",
      "Polarity Score: 0.2452830165539338\n",
      "Subjectivity Score: 0.08848080126170216\n",
      "------------------------------\n",
      "Document 53:\n",
      "Positive Score: 50\n",
      "Negative Score: 5\n",
      "Polarity Score: 0.8181818033057854\n",
      "Subjectivity Score: 0.0924369746345597\n",
      "------------------------------\n",
      "Document 54:\n",
      "Positive Score: 14\n",
      "Negative Score: 3\n",
      "Polarity Score: 0.6470587854671302\n",
      "Subjectivity Score: 0.04038004741002364\n",
      "------------------------------\n",
      "Document 55:\n",
      "Positive Score: 31\n",
      "Negative Score: 10\n",
      "Polarity Score: 0.5121951094586559\n",
      "Subjectivity Score: 0.060294117558391\n",
      "------------------------------\n",
      "Document 56:\n",
      "Positive Score: 22\n",
      "Negative Score: 10\n",
      "Polarity Score: 0.3749999882812504\n",
      "Subjectivity Score: 0.06201550375578391\n",
      "------------------------------\n",
      "Document 57:\n",
      "Positive Score: 24\n",
      "Negative Score: 11\n",
      "Polarity Score: 0.37142856081632686\n",
      "Subjectivity Score: 0.08838383816064688\n",
      "------------------------------\n",
      "Document 58:\n",
      "Positive Score: 14\n",
      "Negative Score: 2\n",
      "Polarity Score: 0.7499999531250029\n",
      "Subjectivity Score: 0.05351170550664981\n",
      "------------------------------\n",
      "Document 59:\n",
      "Positive Score: 32\n",
      "Negative Score: 39\n",
      "Polarity Score: -0.0985915479071613\n",
      "Subjectivity Score: 0.10939907533220482\n",
      "------------------------------\n",
      "Document 60:\n",
      "Positive Score: 18\n",
      "Negative Score: 4\n",
      "Polarity Score: 0.6363636074380178\n",
      "Subjectivity Score: 0.07432432407322863\n",
      "------------------------------\n",
      "Document 61:\n",
      "Positive Score: 42\n",
      "Negative Score: 13\n",
      "Polarity Score: 0.5272727176859506\n",
      "Subjectivity Score: 0.08675078850670223\n",
      "------------------------------\n",
      "Document 62:\n",
      "Positive Score: 10\n",
      "Negative Score: 9\n",
      "Polarity Score: 0.05263157617728546\n",
      "Subjectivity Score: 0.06959706934213528\n",
      "------------------------------\n",
      "Document 63:\n",
      "Positive Score: 18\n",
      "Negative Score: 27\n",
      "Polarity Score: -0.19999999555555567\n",
      "Subjectivity Score: 0.12129380021214609\n",
      "------------------------------\n",
      "Document 64:\n",
      "Positive Score: 35\n",
      "Negative Score: 51\n",
      "Polarity Score: -0.1860465094645755\n",
      "Subjectivity Score: 0.06793048967778002\n",
      "------------------------------\n",
      "Document 65:\n",
      "Positive Score: 51\n",
      "Negative Score: 25\n",
      "Polarity Score: 0.34210525865650976\n",
      "Subjectivity Score: 0.07116104862250837\n",
      "------------------------------\n",
      "Document 66:\n",
      "Positive Score: 43\n",
      "Negative Score: 28\n",
      "Polarity Score: 0.2112676026582028\n",
      "Subjectivity Score: 0.06501831495877443\n",
      "------------------------------\n",
      "Document 67:\n",
      "Positive Score: 33\n",
      "Negative Score: 9\n",
      "Polarity Score: 0.5714285578231296\n",
      "Subjectivity Score: 0.06796116493857417\n",
      "------------------------------\n",
      "Document 68:\n",
      "Positive Score: 49\n",
      "Negative Score: 9\n",
      "Polarity Score: 0.6896551605231869\n",
      "Subjectivity Score: 0.06605922543729018\n",
      "------------------------------\n",
      "Document 69:\n",
      "Positive Score: 39\n",
      "Negative Score: 8\n",
      "Polarity Score: 0.6595744540516074\n",
      "Subjectivity Score: 0.111904761638322\n",
      "------------------------------\n",
      "Document 70:\n",
      "Positive Score: 50\n",
      "Negative Score: 18\n",
      "Polarity Score: 0.47058822837370257\n",
      "Subjectivity Score: 0.05807002556953883\n",
      "------------------------------\n",
      "Document 71:\n",
      "Positive Score: 30\n",
      "Negative Score: 75\n",
      "Polarity Score: -0.42857142448979596\n",
      "Subjectivity Score: 0.14482758600713436\n",
      "------------------------------\n",
      "Document 72:\n",
      "Positive Score: 73\n",
      "Negative Score: 25\n",
      "Polarity Score: 0.48979591336942946\n",
      "Subjectivity Score: 0.11778846139688887\n",
      "------------------------------\n",
      "Document 73:\n",
      "Positive Score: 31\n",
      "Negative Score: 36\n",
      "Polarity Score: -0.074626864557808\n",
      "Subjectivity Score: 0.08292079197658318\n",
      "------------------------------\n",
      "Document 74:\n",
      "Positive Score: 32\n",
      "Negative Score: 25\n",
      "Polarity Score: 0.12280701538935061\n",
      "Subjectivity Score: 0.06462585026686411\n",
      "------------------------------\n",
      "Document 75:\n",
      "Positive Score: 29\n",
      "Negative Score: 79\n",
      "Polarity Score: -0.4629629586762689\n",
      "Subjectivity Score: 0.1461434368793729\n",
      "------------------------------\n",
      "Document 76:\n",
      "Positive Score: 28\n",
      "Negative Score: 13\n",
      "Polarity Score: 0.3658536496133256\n",
      "Subjectivity Score: 0.06984667790486085\n",
      "------------------------------\n",
      "Document 77:\n",
      "Positive Score: 17\n",
      "Negative Score: 8\n",
      "Polarity Score: 0.35999998560000057\n",
      "Subjectivity Score: 0.06738544456230339\n",
      "------------------------------\n",
      "Document 78:\n",
      "Positive Score: 34\n",
      "Negative Score: 20\n",
      "Polarity Score: 0.25925925445816195\n",
      "Subjectivity Score: 0.08181818169421487\n",
      "------------------------------\n",
      "Document 79:\n",
      "Positive Score: 29\n",
      "Negative Score: 34\n",
      "Polarity Score: -0.07936507810531622\n",
      "Subjectivity Score: 0.0675241156832539\n",
      "------------------------------\n",
      "Document 80:\n",
      "Positive Score: 45\n",
      "Negative Score: 13\n",
      "Polarity Score: 0.5517241284185496\n",
      "Subjectivity Score: 0.020855807256074863\n",
      "------------------------------\n",
      "Document 81:\n",
      "Positive Score: 42\n",
      "Negative Score: 50\n",
      "Polarity Score: -0.08695652079395086\n",
      "Subjectivity Score: 0.07924203266215156\n",
      "------------------------------\n",
      "Document 82:\n",
      "Positive Score: 38\n",
      "Negative Score: 64\n",
      "Polarity Score: -0.2549019582852749\n",
      "Subjectivity Score: 0.08651399483756234\n",
      "------------------------------\n",
      "Document 83:\n",
      "Positive Score: 12\n",
      "Negative Score: 3\n",
      "Polarity Score: 0.5999999600000027\n",
      "Subjectivity Score: 0.05454545434710744\n",
      "------------------------------\n",
      "Document 84:\n",
      "Positive Score: 37\n",
      "Negative Score: 6\n",
      "Polarity Score: 0.7209302157923206\n",
      "Subjectivity Score: 0.061340941424620626\n",
      "------------------------------\n",
      "Document 85:\n",
      "Positive Score: 46\n",
      "Negative Score: 36\n",
      "Polarity Score: 0.12195121802498515\n",
      "Subjectivity Score: 0.07481751817991102\n",
      "------------------------------\n",
      "Document 86:\n",
      "Positive Score: 58\n",
      "Negative Score: 16\n",
      "Polarity Score: 0.5675675598977357\n",
      "Subjectivity Score: 0.06281833610966185\n",
      "------------------------------\n",
      "Document 87:\n",
      "Positive Score: 39\n",
      "Negative Score: 35\n",
      "Polarity Score: 0.05405405332359388\n",
      "Subjectivity Score: 0.08894230758540587\n",
      "------------------------------\n",
      "Document 88:\n",
      "Positive Score: 25\n",
      "Negative Score: 46\n",
      "Polarity Score: -0.2957746437214839\n",
      "Subjectivity Score: 0.054447852718981705\n",
      "------------------------------\n",
      "Document 89:\n",
      "Positive Score: 25\n",
      "Negative Score: 44\n",
      "Polarity Score: -0.27536231484982154\n",
      "Subjectivity Score: 0.08914728670652805\n",
      "------------------------------\n",
      "Document 90:\n",
      "Positive Score: 45\n",
      "Negative Score: 50\n",
      "Polarity Score: -0.05263157839335181\n",
      "Subjectivity Score: 0.11390887276509727\n",
      "------------------------------\n",
      "Document 91:\n",
      "Positive Score: 32\n",
      "Negative Score: 29\n",
      "Polarity Score: 0.049180327062617595\n",
      "Subjectivity Score: 0.08068783058110075\n",
      "------------------------------\n",
      "Document 92:\n",
      "Positive Score: 30\n",
      "Negative Score: 43\n",
      "Polarity Score: -0.17808218934133987\n",
      "Subjectivity Score: 0.07128906243038177\n",
      "------------------------------\n",
      "Document 93:\n",
      "Positive Score: 13\n",
      "Negative Score: 5\n",
      "Polarity Score: 0.4444444197530878\n",
      "Subjectivity Score: 0.060200668694981044\n",
      "------------------------------\n",
      "Document 94:\n",
      "Positive Score: 41\n",
      "Negative Score: 50\n",
      "Polarity Score: -0.09890109781427366\n",
      "Subjectivity Score: 0.10520231201710716\n",
      "------------------------------\n",
      "Document 95:\n",
      "Positive Score: 17\n",
      "Negative Score: 27\n",
      "Polarity Score: -0.22727272210743815\n",
      "Subjectivity Score: 0.0705128203998192\n",
      "------------------------------\n",
      "Document 96:\n",
      "Positive Score: 36\n",
      "Negative Score: 59\n",
      "Polarity Score: -0.2421052606094183\n",
      "Subjectivity Score: 0.1103368175257412\n",
      "------------------------------\n",
      "Document 97:\n",
      "Positive Score: 31\n",
      "Negative Score: 37\n",
      "Polarity Score: -0.08823529282006923\n",
      "Subjectivity Score: 0.09353507552471103\n",
      "------------------------------\n",
      "Document 98:\n",
      "Positive Score: 10\n",
      "Negative Score: 2\n",
      "Polarity Score: 0.6666666111111158\n",
      "Subjectivity Score: 0.03809523797430083\n",
      "------------------------------\n",
      "Document 99:\n",
      "Positive Score: 21\n",
      "Negative Score: 5\n",
      "Polarity Score: 0.6153845917159773\n",
      "Subjectivity Score: 0.050290135299245385\n",
      "------------------------------\n",
      "Document 100:\n",
      "Positive Score: 38\n",
      "Negative Score: 56\n",
      "Polarity Score: -0.19148935966500683\n",
      "Subjectivity Score: 0.12651413172743725\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Directories\n",
    "text_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\extracted_articles'\n",
    "stopwords_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\StopWords'\n",
    "sentiment_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\MasterDictionary'\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set()\n",
    "for filename in os.listdir(stopwords_dir):\n",
    "    with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))\n",
    "\n",
    "# Load text files\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "    with open(os.path.join(text_dir, text_file), 'r', encoding='ISO-8859-1') as f:\n",
    "        text = f.read()\n",
    "        # Tokenize the given text file\n",
    "        words = word_tokenize(text)\n",
    "        # Remove the stop words from the tokens\n",
    "        filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "        # Add each filtered token of each file into a list\n",
    "        docs.append(filtered_text)\n",
    "\n",
    "# Store positive and negative words from the directory\n",
    "pos = set()\n",
    "neg = set()\n",
    "\n",
    "for filename in os.listdir(sentiment_dir):\n",
    "    if filename == 'positive-words.txt':\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            pos.update(f.read().splitlines())\n",
    "    else:\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "# Calculate sentiment scores\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_score = []\n",
    "subjectivity_score = []\n",
    "\n",
    "# Iterate through the list of docs\n",
    "for i in range(len(docs)):\n",
    "    positive_words.append([word for word in docs[i] if word.lower() in pos])\n",
    "    negative_words.append([word for word in docs[i] if word.lower() in neg])\n",
    "    positive_score.append(len(positive_words[i]))\n",
    "    negative_score.append(len(negative_words[i]))\n",
    "    polarity_score.append((positive_score[i] - negative_score[i]) / ((positive_score[i] + negative_score[i]) + 0.000001))\n",
    "    subjectivity_score.append((positive_score[i] + negative_score[i]) / ((len(docs[i])) + 0.000001))\n",
    "\n",
    "# Print the sentiment scores\n",
    "for i in range(len(docs)):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Positive Score: {positive_score[i]}\")\n",
    "    print(f\"Negative Score: {negative_score[i]}\")\n",
    "    print(f\"Polarity Score: {polarity_score[i]}\")\n",
    "    print(f\"Subjectivity Score: {subjectivity_score[i]}\")\n",
    "    print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c905c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to Output Data Structure.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Directories\n",
    "text_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\extracted_articles'\n",
    "stopwords_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\StopWords'\n",
    "sentiment_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\MasterDictionary'\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set()\n",
    "for filename in os.listdir(stopwords_dir):\n",
    "    with open(os.path.join(stopwords_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "        stop_words.update(set(f.read().splitlines()))\n",
    "\n",
    "# Store positive and negative words from the directory\n",
    "pos = set()\n",
    "neg = set()\n",
    "\n",
    "for filename in os.listdir(sentiment_dir):\n",
    "    if filename == 'positive-words.txt':\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            pos.update(f.read().splitlines())\n",
    "    else:\n",
    "        with open(os.path.join(sentiment_dir, filename), 'r', encoding='ISO-8859-1') as f:\n",
    "            neg.update(f.read().splitlines())\n",
    "\n",
    "# Initialize lists to store results\n",
    "document_list = []\n",
    "positive_score_list = []\n",
    "negative_score_list = []\n",
    "polarity_score_list = []\n",
    "subjectivity_score_list = []\n",
    "\n",
    "# Process each text file in the articles directory\n",
    "for filename in os.listdir(text_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join(text_dir, filename)\n",
    "        with open(filepath, 'r', encoding='ISO-8859-1') as f:\n",
    "            text = f.read()\n",
    "            # Tokenize the given text file\n",
    "            words = word_tokenize(text)\n",
    "            # Remove the stop words from the tokens\n",
    "            filtered_text = [word for word in words if word.lower() not in stop_words]\n",
    "            \n",
    "            # Calculate sentiment scores\n",
    "            positive_score = sum(1 for word in filtered_text if word.lower() in pos)\n",
    "            negative_score = sum(1 for word in filtered_text if word.lower() in neg)\n",
    "            polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "            subjectivity_score = (positive_score + negative_score) / (len(filtered_text) + 0.000001)\n",
    "\n",
    "            # Append results to lists\n",
    "            document_list.append(filename)\n",
    "            positive_score_list.append(positive_score)\n",
    "            negative_score_list.append(negative_score)\n",
    "            polarity_score_list.append(polarity_score)\n",
    "            subjectivity_score_list.append(subjectivity_score)\n",
    "\n",
    "# Create DataFrame from lists\n",
    "output_df = pd.DataFrame({\n",
    "    'Document': document_list,\n",
    "    'Positive Score': positive_score_list,\n",
    "    'Negative Score': negative_score_list,\n",
    "    'Polarity Score': polarity_score_list,\n",
    "    'Subjectivity Score': subjectivity_score_list\n",
    "})\n",
    "\n",
    "# Save the output DataFrame to Excel file\n",
    "output_file = 'Output Data Structure.xlsx'\n",
    "output_df.to_excel(output_file, index=False)\n",
    "print(f\"Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01100c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables in the analysis document (Text Analysis.docx):\n",
    "#5.\tAVG SENTENCE LENGTH\n",
    "#6.\tPERCENTAGE OF COMPLEX WORDS\n",
    "#7.\tFOG INDEX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dcabaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\Admin\\Desktop\\output\\Output Data Structure1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import pyphen\n",
    "\n",
    "# Directories\n",
    "text_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\extracted_articles'\n",
    "\n",
    "# Output directory\n",
    "output_dir = r'C:\\Users\\Admin\\Desktop\\output'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load text files\n",
    "docs = []\n",
    "for text_file in os.listdir(text_dir):\n",
    "    if text_file.endswith('.txt'):\n",
    "        filepath = os.path.join(text_dir, text_file)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            docs.append((text_file, text))\n",
    "\n",
    "# Initialize lists to store results\n",
    "document_list = []\n",
    "average_sentence_length_list = []\n",
    "percentage_complex_words_list = []\n",
    "fog_index_list = []\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word, dic):\n",
    "    return max(1, len(dic.inserted(word).split(\"-\")))\n",
    "\n",
    "# Initialize pyphen\n",
    "dic = pyphen.Pyphen(lang='en')\n",
    "\n",
    "# Process each document\n",
    "for doc_name, doc_text in docs:\n",
    "    # Tokenize sentences\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    num_sentences = len(sentences)\n",
    "\n",
    "    # Tokenize words\n",
    "    words = word_tokenize(doc_text)\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Count syllables and complex words\n",
    "    num_syllables = sum(count_syllables(word, dic) for word in words)\n",
    "    num_complex_words = sum(1 for word in words if count_syllables(word, dic) > 2)  # Words with more than 2 syllables\n",
    "\n",
    "    # Calculate readability metrics\n",
    "    average_sentence_length = num_words / num_sentences\n",
    "    percentage_complex_words = (num_complex_words / num_words) * 100\n",
    "    fog_index = 0.4 * (average_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Append results to lists\n",
    "    document_list.append(doc_name)\n",
    "    average_sentence_length_list.append(average_sentence_length)\n",
    "    percentage_complex_words_list.append(percentage_complex_words)\n",
    "    fog_index_list.append(fog_index)\n",
    "\n",
    "# Create DataFrame from lists\n",
    "output_df = pd.DataFrame({\n",
    "    'Document': document_list,\n",
    "    'Average Sentence Length': average_sentence_length_list,\n",
    "    'Percentage of Complex Words': percentage_complex_words_list,\n",
    "    'Fog Index': fog_index_list\n",
    "})\n",
    "\n",
    "# Save the output DataFrame to Excel file\n",
    "output_file = os.path.join(output_dir, 'Output Data Structure1.xlsx')\n",
    "output_df.to_excel(output_file, index=False)\n",
    "print(f\"Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e994630",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#variables in the analysis document (Text Analysis.docx):\n",
    "#8.\tAVG NUMBER OF WORDS PER SENTENCE\n",
    "#9.\tCOMPLEX WORD COUNT\n",
    "#10.\tWORD COUNT\n",
    "#11.\tSYLLABLE PER WORD\n",
    "#12.\tPERSONAL PRONOUNS\n",
    "#13.\tAVG WORD LENGTH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c053da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to C:\\Users\\Admin\\Desktop\\output\\Output Data Structure2.xlsx\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the stopwords corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Directories\n",
    "text_dir = r'C:\\Users\\Admin\\Desktop\\black offer\\20211030 Test Assignment\\extracted_articles'\n",
    "\n",
    "# Output directory\n",
    "output_dir = r'C:\\Users\\Admin\\Desktop\\output'\n",
    "\n",
    "\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to count syllables in a word\n",
    "def count_syllables(word):\n",
    "    vowels = 'aeiou'\n",
    "    count = 0\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "# Process each document\n",
    "data = []\n",
    "for filename in os.listdir(text_dir):\n",
    "    if filename.endswith('.txt'):\n",
    "        filepath = os.path.join(text_dir, filename)\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "            # Tokenize sentences\n",
    "            sentences = sent_tokenize(text)\n",
    "            num_sentences = len(sentences)\n",
    "\n",
    "            # Tokenize words\n",
    "            words = word_tokenize(text)\n",
    "            num_words = len(words)\n",
    "\n",
    "            # Remove stop words and punctuations\n",
    "            cleaned_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "\n",
    "            # Average Number of Words Per Sentence\n",
    "            average_words_per_sentence = num_words / num_sentences\n",
    "\n",
    "            # Complex Word Count\n",
    "            complex_word_count = sum(1 for word in cleaned_words if count_syllables(word) > 2)\n",
    "\n",
    "            # Word Count\n",
    "            word_count = len(cleaned_words)\n",
    "\n",
    "            # Syllable Count Per Word\n",
    "            syllable_count_per_word = sum(count_syllables(word) for word in cleaned_words) / len(cleaned_words)\n",
    "\n",
    "            # Personal Pronouns Count\n",
    "            personal_pronouns_count = len(re.findall(r'\\b(?:I|we|my|ours|us)\\b', text))\n",
    "\n",
    "            # Average Word Length\n",
    "            average_word_length = sum(len(word) for word in cleaned_words) / len(cleaned_words)\n",
    "\n",
    "            # Append data to list\n",
    "            data.append({\n",
    "                'Document': filename,\n",
    "                'Average Number of Words Per Sentence': average_words_per_sentence,\n",
    "                'Complex Word Count': complex_word_count,\n",
    "                'Word Count': word_count,\n",
    "                'Syllable Count Per Word': syllable_count_per_word,\n",
    "                'Personal Pronouns': personal_pronouns_count,\n",
    "                'Average Word Length': average_word_length\n",
    "            })\n",
    "\n",
    "# Create DataFrame from the collected data\n",
    "output_df = pd.DataFrame(data)\n",
    "\n",
    "# Save the output DataFrame to Excel file\n",
    "output_file = os.path.join(output_dir, 'Output Data Structure2.xlsx')\n",
    "output_df.to_excel(output_file, index=False)\n",
    "print(f\"Output saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb44d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdf98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
